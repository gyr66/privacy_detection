{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdvfVnDKfTcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430b11e4-274b-441f-d4c0-f1d69c895301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEw0qPY4fW5m",
        "outputId": "f5ba49ea-52b5-45b7-967e-1442fdc115c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/168.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/168.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Collecting accelerate>=0.21.0 (from peft)\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.25.0 peft-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: \"O\",\n",
        "    1: \"B-position\",\n",
        "    2: \"I-position\",\n",
        "    3: \"B-name\",\n",
        "    4: \"I-name\",\n",
        "    5: \"B-movie\",\n",
        "    6: \"I-movie\",\n",
        "    7: \"B-organization\",\n",
        "    8: \"I-organization\",\n",
        "    9: \"B-company\",\n",
        "    10: \"I-company\",\n",
        "    11: \"B-book\",\n",
        "    12: \"I-book\",\n",
        "    13: \"B-address\",\n",
        "    14: \"I-address\",\n",
        "    15: \"B-scene\",\n",
        "    16: \"I-scene\",\n",
        "    17: \"B-mobile\",\n",
        "    18: \"I-mobile\",\n",
        "    19: \"B-email\",\n",
        "    20: \"I-email\",\n",
        "    21: \"B-game\",\n",
        "    22: \"I-game\",\n",
        "    23: \"B-government\",\n",
        "    24: \"I-government\",\n",
        "    25: \"B-QQ\",\n",
        "    26: \"I-QQ\",\n",
        "    27: \"B-vx\",\n",
        "    28: \"I-vx\"\n",
        "  }"
      ],
      "metadata": {
        "id": "GN0_RBfYh5ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {\n",
        "    \"B-QQ\": 25,\n",
        "    \"B-address\": 13,\n",
        "    \"B-book\": 11,\n",
        "    \"B-company\": 9,\n",
        "    \"B-email\": 19,\n",
        "    \"B-game\": 21,\n",
        "    \"B-government\": 23,\n",
        "    \"B-mobile\": 17,\n",
        "    \"B-movie\": 5,\n",
        "    \"B-name\": 3,\n",
        "    \"B-organization\": 7,\n",
        "    \"B-position\": 1,\n",
        "    \"B-scene\": 15,\n",
        "    \"B-vx\": 27,\n",
        "    \"I-QQ\": 26,\n",
        "    \"I-address\": 14,\n",
        "    \"I-book\": 12,\n",
        "    \"I-company\": 10,\n",
        "    \"I-email\": 20,\n",
        "    \"I-game\": 22,\n",
        "    \"I-government\": 24,\n",
        "    \"I-mobile\": 18,\n",
        "    \"I-movie\": 6,\n",
        "    \"I-name\": 4,\n",
        "    \"I-organization\": 8,\n",
        "    \"I-position\": 2,\n",
        "    \"I-scene\": 16,\n",
        "    \"I-vx\": 28,\n",
        "    \"O\": 0\n",
        "  }"
      ],
      "metadata": {
        "id": "jmIvYcMmiBDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512,'return_tensors':'pt'}\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"gyr66/RoBERTa-ext-lora-chinese-finetuned-ner\", trust_remote_code=True)\n",
        "config1 = PeftConfig.from_pretrained(\"gyr66/RoBERTa-ext-lora-chinese-finetuned-ner\")\n",
        "model1 = AutoModelForTokenClassification.from_pretrained(\n",
        "    config1.base_model_name_or_path,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "inference_model1 = PeftModel.from_pretrained(model1, \"gyr66/RoBERTa-ext-lora-chinese-finetuned-ner\")\n",
        "\n",
        "\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"gyr66/RoBERTa-ext-large-lora-chinese-finetuned-ner\", trust_remote_code=True)\n",
        "config2 = PeftConfig.from_pretrained(\"gyr66/RoBERTa-ext-large-lora-chinese-finetuned-ner\")\n",
        "model2 = AutoModelForTokenClassification.from_pretrained(\n",
        "    config2.base_model_name_or_path,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "inference_model2 = PeftModel.from_pretrained(model2, \"gyr66/RoBERTa-ext-large-lora-chinese-finetuned-ner\")\n",
        "\n",
        "\n",
        "tokenizer3 = AutoTokenizer.from_pretrained(\"gyr66/RoBERTa-ext-large-lora-updated-chinese-finetuned-ner\", trust_remote_code=True)\n",
        "config3 = PeftConfig.from_pretrained(\"gyr66/RoBERTa-ext-large-lora-updated-chinese-finetuned-ner\")\n",
        "model3 = AutoModelForTokenClassification.from_pretrained(\n",
        "    config3.base_model_name_or_path,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "inference_model3 = PeftModel.from_pretrained(model3, \"gyr66/RoBERTa-ext-large-lora-updated-chinese-finetuned-ner\")\n",
        "\n",
        "\n",
        "nlp_array = [\n",
        "    pipeline(\"token-classification\",model=\"gyr66/RoBERTa-ext-large-crf-chinese-finetuned-ner\"),\n",
        "    pipeline(\"token-classification\",model=\"gyr66/RoBERTa-ext-large-chinese-finetuned-ner\"),\n",
        "    pipeline(\"token-classification\",model=\"gyr66/bert-base-chinese-finetuned-ner\"),\n",
        "    pipeline(\"token-classification\",model=inference_model1,tokenizer=tokenizer1,trust_remote_code=True),\n",
        "    pipeline(\"token-classification\",model=inference_model2,tokenizer=tokenizer2,trust_remote_code=True),\n",
        "    pipeline(\"token-classification\",model=inference_model3,tokenizer=tokenizer3,trust_remote_code=True),\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbpOKkLRiEH9",
        "outputId": "0808aa0b-264a-4b42-c994-36d90f6720e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The model 'PeftModelForTokenClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n",
            "The model 'PeftModelForTokenClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n",
            "The model 'PeftModelForTokenClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def process_file(filename):\n",
        "    with open(os.path.join('/content/drive/MyDrive/test_last2', filename), 'r') as f:\n",
        "        text = f.read()\n",
        "    result_array = []\n",
        "    for i in range(6):\n",
        "      if len(text) > 512:\n",
        "        truncated_text = text[:512]\n",
        "      else:\n",
        "        truncated_text = text\n",
        "      result_array.append(nlp_array[i](truncated_text))\n",
        "    aggregated_results = count_entity_values0(result_array)\n",
        "    final_results = transform_output(aggregated_results, filename.split('.')[0])\n",
        "    return final_results\n",
        "\n",
        "\n",
        "res = []\n",
        "for filename in sorted(os.listdir('/content/drive/MyDrive/test_last2'), key=lambda x: int(x.split('.')[0])):\n",
        "    file_results = process_file(filename)\n",
        "    res.extend(file_results)\n",
        "    # break\n",
        "\n",
        "df = pd.DataFrame(res)\n",
        "df.to_csv('/content/sample_data/test1/predict.csv', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "72yWVC5uv_hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_output(entities, _id):\n",
        "    result = []\n",
        "    # print(\"flag:\",type)\n",
        "    for entity in entities:\n",
        "        category = entity['entity']\n",
        "        word = entity['word'].replace(' ', '')\n",
        "        start = entity['start']\n",
        "        end = entity['end'] - 1\n",
        "        result.append({\n",
        "            'ID': _id,\n",
        "            'Category': category,\n",
        "            'Pos_b': start,\n",
        "            'Pos_e': end,\n",
        "            'Privacy': word\n",
        "        })\n",
        "    return result"
      ],
      "metadata": {
        "id": "uM-dC1erjYPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def count_entity_values0(list_of_lists):\n",
        "    result = []\n",
        "    max_length = max(len(lst) for lst in list_of_lists)\n",
        "    counters = [Counter() for _ in range(max_length)]\n",
        "    for lst in list_of_lists:\n",
        "        for i, dictionary in enumerate(lst):\n",
        "            counters[i][dictionary['entity']] += 1\n",
        "\n",
        "    # 创建最终的列表\n",
        "    max_len_index = np.argmax([len(lst) for lst in list_of_lists])\n",
        "\n",
        "    for i, counter in enumerate(counters):\n",
        "        entity_most_common = counter.most_common(1)[0][0]\n",
        "\n",
        "        # 使用 'entity' 值最多的字典初始化一个新字典，并使用源列表中的最长列表的 'index', 'word', 'start' and 'end'\n",
        "        new_dict = {\n",
        "            'entity': entity_most_common,\n",
        "            'index': list_of_lists[max_len_index][i]['index'],\n",
        "            'word': list_of_lists[max_len_index][i]['word'],\n",
        "            'start': list_of_lists[max_len_index][i]['start'],\n",
        "            'end': list_of_lists[max_len_index][i]['end'],\n",
        "        }\n",
        "        result.append(new_dict)\n",
        "\n",
        "\n",
        "    # 初始化一个空列表用来保存聚合结果\n",
        "    aggregation_results = []\n",
        "\n",
        "    # 对预测结果进行遍历\n",
        "    for prediction in result:\n",
        "        entity = prediction['entity'][2:] # 去掉前面的标识位置的信息\n",
        "        word = prediction['word']\n",
        "        start = prediction['start']\n",
        "        end = prediction['end']\n",
        "\n",
        "        if len(aggregation_results) == 0 or entity != aggregation_results[-1]['entity'] or (prediction['entity'][0] == 'B' and aggregation_results[-1]['end'] != start - 1):\n",
        "            # 就创建一个新的词汇并将当前预测添加进去\n",
        "            aggregation_results.append({\n",
        "                'entity': entity,\n",
        "                'word': word,\n",
        "                'start': start,\n",
        "                'end': end\n",
        "            })\n",
        "        else:\n",
        "            # 反之，如果实体类型没改变，就将当前预测追加到上一个词汇中\n",
        "            aggregation_results[-1]['word'] += ' ' + word\n",
        "            aggregation_results[-1]['end'] = end\n",
        "    return aggregation_results"
      ],
      "metadata": {
        "id": "6sb4W4VEjcaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}