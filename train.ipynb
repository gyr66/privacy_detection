{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1,2,3,4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "  \n",
    "dataset = load_dataset(\"gyr66/privacy_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2012\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 503\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "dataset[\"validation\"] = dataset.pop(\"test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "# model_checkpoint = \"gyr66/RoBERTa-finetuned-privacy-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    return [-100 if word_id is None else labels[word_id] for word_id in word_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    assert all([len(labels) == len(input_ids) for labels, input_ids in zip(tokenized_inputs[\"labels\"], tokenized_inputs[\"input_ids\"])])\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbdffb5b2384cedaaf3b21880a09de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/2012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1954fee6b404928b60b265ccba87b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    num_proc=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_feature = dataset[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"RoBERTa-finetuned-privacy-detection\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    dataloader_num_workers=16,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgyr679\u001b[0m (\u001b[33mmy-wandb-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yirguo/projects/privacy_detection/wandb/run-20231018_050042-bpd0punw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/my-wandb-team/huggingface/runs/bpd0punw' target=\"_blank\">silver-cloud-12</a></strong> to <a href='https://wandb.ai/my-wandb-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/my-wandb-team/huggingface' target=\"_blank\">https://wandb.ai/my-wandb-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/my-wandb-team/huggingface/runs/bpd0punw' target=\"_blank\">https://wandb.ai/my-wandb-team/huggingface/runs/bpd0punw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73' max='7200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  73/7200 01:11 < 1:59:43, 0.99 it/s, Epoch 1/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.828300</td>\n",
       "      <td>0.383059</td>\n",
       "      <td>0.483280</td>\n",
       "      <td>0.644008</td>\n",
       "      <td>0.552186</td>\n",
       "      <td>0.884491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "Call stack:\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n",
      "    self._run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._process(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n",
      "    self._hm.handle(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 136, in handle\n",
      "    handler(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 144, in handle_request\n",
      "    logger.debug(f\"handle_request: {request_type}\")\n",
      "Message: 'handle_request: status_report'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "Call stack:\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n",
      "    self._run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._process(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n",
      "    self._hm.handle(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 136, in handle\n",
      "    handler(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 144, in handle_request\n",
      "    logger.debug(f\"handle_request: {request_type}\")\n",
      "Message: 'handle_request: stop_status'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "Call stack:\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n",
      "    self._run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._process(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 328, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 376, in send\n",
      "    send_handler(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 396, in send_request\n",
      "    logger.debug(f\"send_request: {request_type}\")\n",
      "Message: 'send_request: stop_status'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "Call stack:\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n",
      "    self.dispatch_events(self.event_queue, self.timeout)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n",
      "    handler.dispatch(event)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n",
      "    _method_map[event_type](event)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 272, in _on_file_created\n",
      "    logger.info(\"file/dir created: %s\", event.src_path)\n",
      "Message: 'file/dir created: %s'\n",
      "Arguments: ('/home/yirguo/projects/privacy_detection/wandb/run-20231018_050042-bpd0punw/files/output.log',)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "Call stack:\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n",
      "    self._run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n",
      "    self._process(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n",
      "    self._hm.handle(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 136, in handle\n",
      "    handler(record)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 144, in handle_request\n",
      "    logger.debug(f\"handle_request: {request_type}\")\n",
      "Message: 'handle_request: status_report'\n",
      "Arguments: ()\n",
      "Exception in thread OutRawRd-stderr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1205, in _output_raw_reader_thread\n",
      "    self._output_raw_flush(stream)\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1220, in _output_raw_flush\n",
      "    self._output_raw_file.write(data.encode(\"utf-8\"))\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 79, in write\n",
      "    super().write(b\"\\n\".join(ret) + b\"\\n\")\n",
      "  File \"/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 46, in write\n",
      "    self.f.flush()\n",
      "OSError: [Errno 5] Input/output error\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/transformers/trainer.py:1994\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1991\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1994\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1996\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   1997\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1998\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[1;32m   2241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/transformers/trainer.py:2297\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2295\u001b[0m run_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_dir(trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m   2296\u001b[0m output_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 2297\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_model(output_dir, _internal_call\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   2298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[1;32m   2299\u001b[0m     \u001b[39m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[39;00m\n\u001b[1;32m   2300\u001b[0m     \u001b[39m# config `stage3_gather_16bit_weights_on_model_save` is True\u001b[39;00m\n\u001b[1;32m   2301\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n",
      "File \u001b[0;32m/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/transformers/trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2773\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2775\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[0;32m-> 2776\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save(output_dir)\n\u001b[1;32m   2778\u001b[0m \u001b[39m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2779\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/transformers/trainer.py:2830\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(output_dir, state_dict\u001b[39m=\u001b[39mstate_dict)\n\u001b[1;32m   2829\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2830\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49msave_pretrained(output_dir)\n\u001b[1;32m   2832\u001b[0m \u001b[39m# Good practice: save your training arguments together with the trained model\u001b[39;00m\n\u001b[1;32m   2833\u001b[0m torch\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, TRAINING_ARGS_NAME))\n",
      "File \u001b[0;32m/mnt/ds3lab-scratch/yirguo/miniconda3/envs/ADMM/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2160\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.save_pretrained\u001b[0;34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m   2158\u001b[0m     tokenizer_config\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2160\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(tokenizer_config_file, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m   2161\u001b[0m     out_str \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(tokenizer_config, indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, sort_keys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2162\u001b[0m     f\u001b[39m.\u001b[39mwrite(out_str)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/gyr66/RoBERTa-finetuned-privacy-detection\n",
      "   1e68d77..e4a7c04  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training 100 Epochs and save the best F1 model\", language=\"zh\", dataset_tags=\"gyr66/privacy_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('ADMM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "812773d9390504bc29ef9988f01e5f691fe2c7dd3271bdfe9a90bc479fcbe9db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
